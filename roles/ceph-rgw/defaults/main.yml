---
# You can override vars by using host or group vars

###########
# GENERAL #
###########

fetch_directory: fetch/

# Even though RGW nodes should not have the admin key
# at their disposal, some people might want to have it
# distributed on RGW nodes. Setting 'copy_admin_key' to 'true'
# will copy the admin key to the /etc/ceph/ directory
copy_admin_key: false

## Ceph options
#
cephx: true

# Multi-site remote pull URL variables
rgw_pull_port: "{{ radosgw_civetweb_port }}"
rgw_pull_proto: "http"

########
#TUNING#
########
# The below values were determined to be safe defaults to 
# prevent users from putting too many objects into their
# buckets which causes performance and stability issues.
#ceph_conf_overrides:
#    rgw:
#      radosgw_civetweb_num_threads: 100
#      rgw_override_bucket_index_max_shards: 16
#      rgw_bucket_default_quota_max_objects: 1638400 # i.e., 100K * 16

# Fill in the below values (rgw_pool_nameX & pg_num) to 
# create pools for the rgw with pg_num of placement groups
#rgw_pool:
#  rgw_pool_name1:
#    pg_num: y1
#  rgw_pool_name2:
#    pg_num: y2
#  rgw_pool_name3:
#    pg_num: y3

##########
# DOCKER #
##########

rgw_containerized_deployment: false
rgw_containerized_deployment_with_kv: false
kv_type: etcd
kv_endpoint: 127.0.0.1
ceph_rgw_civetweb_port: "{{ radosgw_civetweb_port }}"
ceph_docker_image: "ceph/daemon"
ceph_docker_image_tag: latest
ceph_rgw_docker_extra_env: -e CLUSTER={{ cluster }} -e RGW_CIVETWEB_PORT={{ ceph_rgw_civetweb_port }}
ceph_docker_on_openstack: false
ceph_config_keys: [] # DON'T TOUCH ME
rgw_config_keys: "/" # DON'T TOUCH ME
